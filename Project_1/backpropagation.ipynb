{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tests_backpropagation import main_test\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class ``MyNet``\n",
    "\n",
    "Read carefully how ``MyNet`` is implemented in the cell below. In particular:  \n",
    "- ``n_hid`` is a list of integer, representing the number of hidden units in each hidden layer.   \n",
    "-  ``MyNet([2, 3, 2]) = MiniNet()`` where ``MiniNet`` is the neural network defined in the fourth tutorial, in which notations are also clarified.     \n",
    "- ``model.L`` is the number of hidden layers, ``L``   \n",
    "- ``model.f[l]`` is the activation function of layer ``l``, $f^{[l]}$ (here ``torch.tanh``)   \n",
    "- ``model.df[l]`` is the derivative of the activation function, $f'^{[l]}$   \n",
    "- ``model.a[l]``  is the tensor $A^{[l]}$, (shape: ``(1, n(l))``)   \n",
    "- ``model.z[l]``  is the tensor $Z^{[l]}$, (shape: ``(1, n(l))``)  \n",
    "- Weights $W^{[l]}$ (shape: ``(n(l+1), n(l))``) and biases $\\mathbf{b}^{[l]}$ (shape: ``(n(l+1))``) can be accessed as follows:\n",
    "```\n",
    "weights = model.fc[str(l)].weight.data\n",
    "bias = model.fc[str(l)].bias.data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, n_l = [2, 3, 2]):\n",
    "        super().__init__() \n",
    "        \n",
    "        \n",
    "        # number of layers in our network (following Andrew's notations)\n",
    "        self.L = len(n_l)-1\n",
    "        self.n_l = n_l\n",
    "        \n",
    "        # Where we will store our neuron values\n",
    "        # - z: before activation function \n",
    "        # - a: after activation function (a=f(z))\n",
    "        self.z = {i : None for i in range(1, self.L+1)}\n",
    "        self.a = {i : None for i in range(self.L+1)}\n",
    "\n",
    "        # Where we will store the gradients for our custom backpropagation algo\n",
    "        self.dL_dw = {i : None for i in range(1, self.L+1)}\n",
    "        self.dL_db = {i : None for i in range(1, self.L+1)}\n",
    "\n",
    "        # Our activation functions\n",
    "        self.f = {i : lambda x : torch.tanh(x) for i in range(1, self.L+1)}\n",
    "\n",
    "        # Derivatives of our activation functions\n",
    "        self.df = {\n",
    "            i : lambda x : (1 / (torch.cosh(x)**2)) \n",
    "            for i in range(1, self.L+1)\n",
    "        }\n",
    "        \n",
    "        # fully connected layers\n",
    "        # We have to use nn.ModuleDict and to use strings as keys here to \n",
    "        # respect pytorch requirements (otherwise, the model does not learn)\n",
    "        self.fc = nn.ModuleDict({str(i): None for i in range(1, self.L+1)})\n",
    "        for i in range(1, self.L+1):\n",
    "            self.fc[str(i)] = nn.Linear(in_features=n_l[i-1], out_features=n_l[i])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "        \n",
    "        # Hidden layers until output layer\n",
    "        for i in range(1, self.L+1):\n",
    "\n",
    "            # fully connected layer\n",
    "            self.z[i] = self.fc[str(i)](self.a[i-1])\n",
    "            # activation\n",
    "            self.a[i] = self.f[i](self.z[i])\n",
    "\n",
    "        # return output\n",
    "        return self.a[self.L]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Write a function ``backpropagation(model, y_true, y_pred)`` that computes:\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial w^{[l]}_{i,j}}$ and store them in ``model.dL_dw[l][i,j]`` for $l \\in [1 .. L]$ \n",
    "- $\\frac{\\partial L}{\\partial b^{[l]}_{j}}$ and store them in ``model.dL_db[l][j]`` for $l \\in [1 .. L]$ \n",
    "\n",
    "assuming ``model`` is an instance of the ``MyNet`` class.\n",
    "\n",
    "A vectorized implementation would be appreciated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(model: MyNet, y_true, y_pred):\n",
    "    #model.fc[model.L]\n",
    "\n",
    "    #Bias in last layer\n",
    "    db = (-2*(y_true - y_pred)) * model.df[model.L](model.z[model.L])\n",
    "    model.dL_db[model.L] = db[0]\n",
    "\n",
    "    #print(f'bias: {model.fc[str(model.L)].bias.data}')\n",
    "    #print(f'dbias: {model.dL_db[model.L].data}')\n",
    "    \n",
    "    #Weight in last layer\n",
    "    model.dL_dw[model.L] = (model.a[model.L-1].T @ db).T\n",
    "\n",
    "    #print(f'weights: {model.fc[str(model.L)].weight.data}')\n",
    "    #print(f'dweights: {model.dL_dw[model.L].data}')\n",
    "\n",
    "\n",
    "    for l in range(model.L-1, 0, -1):\n",
    "        #Update bias\n",
    "        db = (((model.fc[str(l+1)].weight.data).T @ (db.T)).T * model.df[l](model.z[l]))\n",
    "        model.dL_db[l] = db[0]\n",
    "\n",
    "        #print(f'bias: {model.fc[str(l)].bias.data}')\n",
    "        #print(f'dbias: {model.dL_db[l].data}')\n",
    "\n",
    "        #Update weight\n",
    "        model.dL_dw[l] = (model.a[l-1].T @ db).T\n",
    "\n",
    "        #print(f'weights: {model.fc[str(l)].weight.data}')\n",
    "        #print(f'dweights: {model.dL_dw[l].data}')\n",
    "\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cells below, and check the output\n",
    "\n",
    "- In the 1st cell, we use a toy dataset and the same architecture as the MiniNet class of the fourth tutorial. \n",
    "- In the 2nd cell, we use a few samples of the MNIST dataset with a consistent model architecture (``24x24`` black and white cropped images as input and ``10`` output classes). \n",
    "\n",
    "You can set ``verbose`` to ``True`` if you want more details about your computations versus what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ========================  Check gradients ======================== \n",
      "tensor([[ 0.7641, -0.6987]], grad_fn=<MulBackward0>)\n",
      "tensor([ 0.7641, -0.6987], grad_fn=<SelectBackward0>)\n",
      "tensor([[-1.1053,  0.6603]], grad_fn=<MulBackward0>)\n",
      "tensor([-1.1053,  0.6603], grad_fn=<SelectBackward0>)\n",
      "tensor([[-3.9959,  4.0016]], grad_fn=<MulBackward0>)\n",
      "tensor([-3.9959,  4.0016], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.5633,  0.7045]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.5633,  0.7045], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.5718,  0.6346]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.5718,  0.6346], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.4721,  0.5259]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.4721,  0.5259], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.4020,  0.4352]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.4020,  0.4352], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.3495,  0.3701]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.3495,  0.3701], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.3088,  0.3220]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.3088,  0.3220], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.2760,  0.2850]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.2760,  0.2850], grad_fn=<SelectBackward0>)\n",
      "\n",
      " ================= Epoch 1 ================= \n",
      "\n",
      " ------------ fc['1'].weight.grad ------------ \n",
      "  Our computation:\n",
      " tensor([[ 2.3204e-07,  2.7189e-07],\n",
      "        [-1.1167e-03, -1.3085e-03],\n",
      "        [ 1.0170e-05,  1.1916e-05]], grad_fn=<PermuteBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([[ 2.3204e-07,  2.7189e-07],\n",
      "        [-1.1167e-03, -1.3085e-03],\n",
      "        [ 1.0170e-05,  1.1916e-05]])\n",
      "\n",
      " ------------- fc['1'].bias.grad ------------- \n",
      "  Our computation:\n",
      " tensor([ 2.9556e-08, -1.4224e-04,  1.2954e-06], grad_fn=<SelectBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([ 2.9556e-08, -1.4224e-04,  1.2954e-06])\n",
      "\n",
      " ------------- relative error ------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad, model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad, model.dL_db[2]):   0.0000\n",
      "Gradients consistent with finite differences computations. :) \n",
      "tensor([[ 0.7665, -0.6516]], grad_fn=<MulBackward0>)\n",
      "tensor([ 0.7665, -0.6516], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0592,  0.0248]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0592,  0.0248], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0446,  0.0411]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0446,  0.0411], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0619,  0.0619]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0619,  0.0619], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0905,  0.0897]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0905,  0.0897], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.1035,  0.1054]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.1035,  0.1054], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.1161,  0.1191]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.1161,  0.1191], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.1264,  0.1295]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.1264,  0.1295], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.1332,  0.1362]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.1332,  0.1362], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.1368,  0.1396]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.1368,  0.1396], grad_fn=<SelectBackward0>)\n",
      "\n",
      " ================= Epoch 2 ================= \n",
      "\n",
      " ------------ fc['1'].weight.grad ------------ \n",
      "  Our computation:\n",
      " tensor([[ 4.6141e-08,  5.4066e-08],\n",
      "        [-3.0342e-04, -3.5553e-04],\n",
      "        [ 1.6560e-06,  1.9404e-06]], grad_fn=<PermuteBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([[ 4.6141e-08,  5.4066e-08],\n",
      "        [-3.0342e-04, -3.5553e-04],\n",
      "        [ 1.6560e-06,  1.9404e-06]])\n",
      "\n",
      " ------------- fc['1'].bias.grad ------------- \n",
      "  Our computation:\n",
      " tensor([ 5.8773e-09, -3.8648e-05,  2.1093e-07], grad_fn=<SelectBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([ 5.8773e-09, -3.8648e-05,  2.1093e-07])\n",
      "\n",
      " ------------- relative error ------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad, model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad, model.dL_db[2]):   0.0000\n",
      "Gradients consistent with finite differences computations. :) \n",
      "tensor([[ 0.6681, -0.7635]], grad_fn=<MulBackward0>)\n",
      "tensor([ 0.6681, -0.7635], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0438,  0.0197]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0438,  0.0197], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0242,  0.0228]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0242,  0.0228], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0330,  0.0333]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0330,  0.0333], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0487,  0.0489]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0487,  0.0489], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0584,  0.0597]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0584,  0.0597], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0680,  0.0698]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0680,  0.0698], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0771,  0.0791]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0771,  0.0791], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0845,  0.0866]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0845,  0.0866], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0902,  0.0923]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0902,  0.0923], grad_fn=<SelectBackward0>)\n",
      "\n",
      " ================= Epoch 3 ================= \n",
      "\n",
      " ------------ fc['1'].weight.grad ------------ \n",
      "  Our computation:\n",
      " tensor([[ 9.9041e-09,  1.1605e-08],\n",
      "        [-1.0512e-04, -1.2318e-04],\n",
      "        [ 3.6825e-07,  4.3149e-07]], grad_fn=<PermuteBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([[ 9.9041e-09,  1.1605e-08],\n",
      "        [-1.0512e-04, -1.2318e-04],\n",
      "        [ 3.6825e-07,  4.3149e-07]])\n",
      "\n",
      " ------------- fc['1'].bias.grad ------------- \n",
      "  Our computation:\n",
      " tensor([ 1.2615e-09, -1.3390e-05,  4.6906e-08], grad_fn=<SelectBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([ 1.2615e-09, -1.3390e-05,  4.6906e-08])\n",
      "\n",
      " ------------- relative error ------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad, model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad, model.dL_db[2]):   0.0000\n",
      "Gradients consistent with finite differences computations. :) \n",
      "tensor([[ 0.1551, -0.6351]], grad_fn=<MulBackward0>)\n",
      "tensor([ 0.1551, -0.6351], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0311,  0.0173]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0311,  0.0173], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0163,  0.0154]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0163,  0.0154], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0221,  0.0220]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0221,  0.0220], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0327,  0.0325]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0327,  0.0325], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0401,  0.0403]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0401,  0.0403], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0475,  0.0479]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0475,  0.0479], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0547,  0.0552]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0547,  0.0552], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0611,  0.0616]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0611,  0.0616], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0665,  0.0670]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0665,  0.0670], grad_fn=<SelectBackward0>)\n",
      "\n",
      " ================= Epoch 4 ================= \n",
      "\n",
      " ------------ fc['1'].weight.grad ------------ \n",
      "  Our computation:\n",
      " tensor([[ 3.8036e-09,  4.4568e-09],\n",
      "        [-5.1638e-05, -6.0507e-05],\n",
      "        [ 1.4991e-07,  1.7565e-07]], grad_fn=<PermuteBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([[ 3.8036e-09,  4.4568e-09],\n",
      "        [-5.1638e-05, -6.0507e-05],\n",
      "        [ 1.4991e-07,  1.7565e-07]])\n",
      "\n",
      " ------------- fc['1'].bias.grad ------------- \n",
      "  Our computation:\n",
      " tensor([ 4.8449e-10, -6.5775e-06,  1.9094e-08], grad_fn=<SelectBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([ 4.8449e-10, -6.5775e-06,  1.9094e-08])\n",
      "\n",
      " ------------- relative error ------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad, model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad, model.dL_db[2]):   0.0000\n",
      "Gradients consistent with finite differences computations. :) \n",
      "tensor([[-0.1774, -0.2802]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.1774, -0.2802], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0214,  0.0143]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0214,  0.0143], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0124,  0.0116]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0124,  0.0116], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0170,  0.0164]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0170,  0.0164], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0252,  0.0244]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0252,  0.0244], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0311,  0.0304]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0311,  0.0304], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0372,  0.0364]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0372,  0.0364], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0432,  0.0423]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0432,  0.0423], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0487,  0.0477]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0487,  0.0477], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0535,  0.0525]], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0535,  0.0525], grad_fn=<SelectBackward0>)\n",
      "\n",
      " ================= Epoch 5 ================= \n",
      "\n",
      " ------------ fc['1'].weight.grad ------------ \n",
      "  Our computation:\n",
      " tensor([[ 2.8513e-09,  3.3409e-09],\n",
      "        [-3.6105e-05, -4.2305e-05],\n",
      "        [ 1.0414e-07,  1.2203e-07]], grad_fn=<PermuteBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([[ 2.8513e-09,  3.3409e-09],\n",
      "        [-3.6105e-05, -4.2305e-05],\n",
      "        [ 1.0414e-07,  1.2203e-07]])\n",
      "\n",
      " ------------- fc['1'].bias.grad ------------- \n",
      "  Our computation:\n",
      " tensor([ 3.6318e-10, -4.5989e-06,  1.3265e-08], grad_fn=<SelectBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([ 3.6318e-10, -4.5989e-06,  1.3265e-08])\n",
      "\n",
      " ------------- relative error ------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad, model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad, model.dL_db[2]):   0.0000\n",
      "Gradients consistent with finite differences computations. :) \n",
      "\n",
      " ==============  Check that weights have been updated ============= \n",
      "tensor([[-0.8314, -0.3591],\n",
      "        [ 0.2055,  0.4616],\n",
      "        [-0.4253, -0.6251]])\n",
      "tensor([-0.5126, -0.4255,  0.4362])\n",
      "Weights have been updated. :)\n",
      "\n",
      " ===================  Check computational graph =================== \n",
      "All parameters seem correctly attached to the computational graph! :) \n"
     ]
    }
   ],
   "source": [
    "model = MyNet([2, 3, 2])\n",
    "\n",
    "main_test(backpropagation, model, verbose=True, data='toy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ========================  Check gradients ======================== \n",
      "\n",
      " ================= Epoch 1 ================= \n",
      "Gradients consistent with autograd's computations. :) \n",
      "Gradients consistent with finite differences computations. :) \n",
      "\n",
      " ==============  Check that weights have been updated ============= \n",
      "Weights have been updated. :)\n",
      "\n",
      " ===================  Check computational graph =================== \n",
      "All parameters seem correctly attached to the computational graph! :) \n"
     ]
    }
   ],
   "source": [
    "model = MyNet([24*24, 16, 10])\n",
    "main_test(backpropagation, model, verbose=False, data='mnist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('nglm-env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ae36e8c2cbd9e14d80419493f2540eab6c211be174ac39ce04705a74740d0d8b"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
